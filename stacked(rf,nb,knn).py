# -*- coding: utf-8 -*-
"""stacked(Rf,NB,KNN).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Diq0WOLE8Ddbmm8NTFNLBEGY96h5ESm0
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mlxtend.plotting import plot_confusion_matrix
from mlxtend.classifier import StackingClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler,LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix,classification_report, f1_score
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
import seaborn as sns

import io
from google.colab import files
uploaded = files.upload()
data= pd.read_csv(io.BytesIO(uploaded['fertilizer (7).csv']))
data.head()

X = data.drop('Fertilizer Name', axis = 1)
y = data['Fertilizer Name']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

data['Soil Type'].unique()
data["Crop Type"].unique()
data["Fertilizer Name"].unique()
soil_type_label_encoder = LabelEncoder()
data["Soil Type"] = soil_type_label_encoder.fit_transform(data["Soil Type"])
crop_type_label_encoder = LabelEncoder()
data["Crop Type"] = crop_type_label_encoder.fit_transform(data["Crop Type"])
fertname_label_encoder = LabelEncoder()
data["Fertilizer Name"] = fertname_label_encoder.fit_transform(data["Fertilizer Name"])
data.head()

no_zero=['Potassium','Phosphorous']
for column in no_zero:
 data[column]=data[column].replace(0,np.NaN)
 mean=int(data[column].mean(skipna=True))
 data[column]=data[column].replace(np.NaN,mean)
#split_the_dataset_to_test_and_training
X=data.iloc[:,0:3]
y=data.iloc[:,3]
X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0,test_size=0.10)

sc_X=StandardScaler()
X_train=sc_X.fit_transform(X_train)
X_test=sc_X.transform(X_test)

classifier=KNeighborsClassifier(n_neighbors=9,p=7,metric='euclidean',weights='uniform')

classifier.fit(X_train,y_train)

y_pred=classifier.predict(X_test)
y_pred

print(classification_report(y_test,y_pred))
print(f1_score(y_pred,y_test,average='weighted'))
print(accuracy_score(y_pred,y_test))

soiltype = data['Soil Type'].tolist()
print("Soiltype:", soiltype)
croptype = data['Crop Type'].tolist()
print("Croptype:", croptype)
fertilizername = data['Fertilizer Name'].tolist()
print("Fertilizer:", fertilizername)

from sklearn import preprocessing
le = preprocessing.LabelEncoder()
st_encoded = le.fit_transform(soiltype)
print(st_encoded)
ct_encoded = le.fit_transform(croptype)
print(ct_encoded)
label = le.fit_transform(fertilizername)
print(label)
feature = zip(st_encoded, ct_encoded)
feature = list(feature)
print(tuple(feature))

# Import Gaussian Naive Bayes model
from sklearn.naive_bayes import GaussianNB
# Create a Gaussian Classifier
model = GaussianNB()
# Train the model using the training sets
model.fit(feature, label.reshape(-1, 1))
# Predict Output
predicted = model.predict([[2, 1]])
a = le.inverse_transform(predicted)
print("PREDICTED FERTILIZER:", a)

# Import train_test_split function
from sklearn.model_selection import train_test_split
# Split dataset into training set and test set
X_train, X_test, y_train, y_test = train_test_split(feature, label, test_size=0.2,
 random_state=90) # 70% training and 30% test
# Import Gaussian Naive Bayes model
from sklearn.naive_bayes import GaussianNB
# Create a Gaussian Classifier
gnb = GaussianNB()
# Train the model using the training sets
gnb.fit(X_train, y_train)
# Predict the response for test dataset
y_pred = gnb.predict(X_test)
# Import scikit-learn metrics module for accuracy calculation
from sklearn import metrics
# Model Accuracy, how often is the classifier correct?
print("Accuracy:", metrics.accuracy_score(y_test, y_pred))

from sklearn.metrics import f1_score
print(f1_score(y_pred,y_test,average='weighted'))
from sklearn.metrics import classification_report,confusion_matrix
print(classification_report(le.inverse_transform(y_test),le.inverse_transform(y_pred)))

#Seperate the Target variable
X = data.drop(['Fertilizer Name'],axis=1)
y = data['Fertilizer Name']
#Spliting Dataset into Test and Train
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=90)

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=90)
#Standardisation
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
classifier = RandomForestClassifier(n_estimators= 20, criterion = 'gini' , random_state= 90)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
print(fertname_label_encoder.inverse_transform(y_pred))
print("Accuracy: {:.2f}%".format(accuracy_score(y_test, y_pred) * 100))
plt.figure(figsize = (15,9))

KNC = KNeighborsClassifier()
NB = GaussianNB()
RF = RandomForestClassifier()

lr = LogisticRegression()  # defining meta-classifier
clf_stack = StackingClassifier(classifiers =[KNC, NB,RF], meta_classifier = lr, use_probas = True, use_features_in_secondary = True)

model_stack = clf_stack.fit(X_train, y_train)   # training of stacked model
pred_stack = model_stack.predict(X_test)

acc_stack = accuracy_score(y_test, pred_stack)  # evaluating accuracy
print('accuracy score of Stacked model:', acc_stack * 100)

sns.heatmap(confusion_matrix(y_test, pred_stack), annot = True)
plt.title("Confusion Matrix")
plt.show()

print(classification_report(fertname_label_encoder.inverse_transform(y_test),fertname_label_encoder.
inverse_transform(pred_stack)))